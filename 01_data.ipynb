{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai2_resnet_audio.model import * \n",
    "from fastai2.torch_core import TensorBase\n",
    "from fastai2.data.block import TransformBlock\n",
    "from fastcore.transform import Transform\n",
    "from fastai2.vision.augment import RandTransform\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastai2-resnet-audio data\n",
    "\n",
    "> DataBlock and transforms for fastai2-resnet-audio model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBlock\n",
    "\n",
    "AudioBlock creates a TensorAudio instance. TensorAudio uses torchaudio to load the sound file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def AudioBlock():\n",
    "    return TransformBlock(type_tfms=TensorAudio.create, batch_tfms=None)\n",
    "\n",
    "class TensorAudio(TensorBase):\n",
    "          \n",
    "    @classmethod\n",
    "    def create(cls, o, norm=True):\n",
    "        o, sr = torchaudio.load(o, normalization=norm)\n",
    "        o = cls(o)\n",
    "        o.sr = sr\n",
    "        o.mode = 'raw'\n",
    "        return o\n",
    "    '''\n",
    "    def show(self, ctx=None):\n",
    "        if self.mode == 'raw':\n",
    "            print(self.shape)\n",
    "            librosa.display.waveplot(np.asarray(self.squeeze()), sr=self.sr)\n",
    "            #print(img.shape)\n",
    "    '''   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class AudioFixLength(Transform):\n",
    "\n",
    "    def __init__(self, length=0.0):\n",
    "        self.length = length\n",
    "\n",
    "    def encodes(self, o: TensorAudio):\n",
    "        if self.length > 0.0:\n",
    "            n_samples = int(o.sr * self.length)\n",
    "            if n_samples < len(o.squeeze()):\n",
    "                o = torch.split(o, n_samples, dim=1)[0]\n",
    "            else:\n",
    "                n_pad = int(o.sr * self.length - len(o.squeeze()))\n",
    "                n_pre = (torch.rand(1) * n_pad).int()\n",
    "                n_post = n_pad - n_pre\n",
    "                o = F.pad(input=o, pad=(n_pre,n_post), mode='constant', value=0)\n",
    "        return o\n",
    "    \n",
    "class AudioResample(Transform):\n",
    "\n",
    "    def __init__(self, target_sr=0, device='cpu'):\n",
    "        self.target_sr = target_sr\n",
    "        self.device = device\n",
    "\n",
    "    def encodes(self, o: TensorAudio):\n",
    "        if self.target_sr != o.sr:\n",
    "            resample = torchaudio.transforms.Resample(orig_freq=o.sr, new_freq=self.target_sr)\n",
    "            o = TensorAudio(resample(o))\n",
    "            o.sr = self.target_sr\n",
    "        return o\n",
    "    \n",
    "class AudioToMono(Transform):\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "\n",
    "    def encodes(self, o: TensorAudio):\n",
    "        sr = o.sr\n",
    "        o = TensorAudio(torch.mean(o,dim=0).unsqueeze(0))\n",
    "        o.sr = sr\n",
    "        return o\n",
    "\n",
    "    \n",
    "class AudioRandomCrop(RandTransform):\n",
    "    \n",
    "    def __init__(self, p=1.0, length=0.0): \n",
    "        super().__init__(p=p)\n",
    "        self.length = length\n",
    "        \n",
    "    def encodes(self, o: TensorAudio): \n",
    "        if self.length > 0.0:\n",
    "            n_samples = int(o.sr * self.length)\n",
    "            if n_samples < len(o[0]):\n",
    "                n_cut = len(o[0]) - n_samples\n",
    "                n_pre = (n_cut * torch.rand(1)).int()\n",
    "                o = o[:,n_pre:(n_samples + n_pre)]\n",
    "        return o    \n",
    "    \n",
    "\n",
    "class AudioAddNoise(RandTransform):\n",
    "    \"Randomly add noise with probability `p`\"\n",
    "    def __init__(self, p=0.5, device='cpu'): \n",
    "        super().__init__(p=p)\n",
    "        self.device=device\n",
    "        \n",
    "    def encodes(self, o: TensorAudio): \n",
    "        noise_amp = (0.001*torch.rand(1) * torch.max(o)).to(self.device)\n",
    "        o = o + noise_amp * torch.empty(o.shape).normal_().to(self.device)\n",
    "        return o\n",
    "    \n",
    "\n",
    "class AudioToTensor(Transform):\n",
    "\n",
    "    def encodes(self, o: TensorAudio):\n",
    "        o = tensor(o).float()\n",
    "        return o\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioFixLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# 0.5 second sample at 44100 Hz\n",
    "ta1 = TensorAudio(torch.randn(1,22050))\n",
    "ta1.sr = 44100\n",
    "assert ta1.shape == (1,22050)\n",
    "\n",
    "# Test AudioFixLength\n",
    "# 2.0 second sample at 44100 Hz\n",
    "ta2 = TensorAudio(torch.randn(1,88200))\n",
    "ta2.sr = 44100\n",
    "assert ta2.shape == (1,88200)\n",
    "\n",
    "# set length to 1.0 seconds -> 44100 samples\n",
    "tfm = AudioFixLength(1.0)\n",
    "ta1 = tfm.encodes(ta1)\n",
    "assert ta1.shape == (1,44100)\n",
    "\n",
    "ta2 = tfm.encodes(ta2)\n",
    "assert ta2.shape == (1,44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioResample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# 1 second sample at 44100 Hz\n",
    "ta1 = TensorAudio(torch.randn(1,44100))\n",
    "ta1.sr = 44100\n",
    "assert ta1.shape == (1,44100)\n",
    "\n",
    "# resample to 22050 Hz\n",
    "tfm = AudioResample(target_sr=22050)\n",
    "ta1 = tfm.encodes(ta1)\n",
    "\n",
    "assert ta1.shape == (1,22050)\n",
    "assert ta1.sr == 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioToMono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# 1 second 2 channel sample at 44100 Hz \n",
    "ta1 = TensorAudio(torch.empty(2,44100))\n",
    "ta1.sr = 44100\n",
    "ta1[0].fill_(0.)\n",
    "ta1[1].fill_(1.)\n",
    "assert ta1.shape == (2,44100)\n",
    "\n",
    "tfm = AudioToMono()\n",
    "ta1 = tfm.encodes(ta1)\n",
    "\n",
    "assert ta1.shape == (1,44100)\n",
    "assert ta1[0][0] == 0.5\n",
    "assert ta1.mean() == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('fastai2': conda)",
   "language": "python",
   "name": "python38164bitfastai2conda8f00f3245bb84569a74bc3f339ede271"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
